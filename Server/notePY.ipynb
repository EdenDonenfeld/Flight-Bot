{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from JSON file\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    sentences = [item['request'] for item in data]  # Extracting sentences from the 'request' key\n",
    "    labels = [np.argmax([item[key] for key in ['is_order', 'is_refund', 'is_status', 'is_date_change', 'is_dest_change', 'is_weather', 'is_allowed']]) for item in data]\n",
    "    return sentences, np.array(labels)  # Return sentences as a list of strings\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "sentences, labels = load_data(\"data/data.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode data\n",
    "max_length = 64  # Adjust according to your data and BERT model's maximum input length\n",
    "encoded_data = tokenizer(sentences, padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
    "\n",
    "# Convert BatchEncoding to dictionary\n",
    "encoded_data = {key: encoded_data[key].numpy() for key in encoded_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numpy array\n",
    "# labels = np.array(labels)\n",
    "# print(labels)\n",
    "# Convert one-hot encoded labels to integer indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and testing\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(encoded_data['input_ids'], labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model\n",
    "num_classes = 7 \n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with sparse categorical cross-entropy loss\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = ['accuracy']\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 150s 18s/step - loss: 1.9712 - accuracy: 0.1222 - val_loss: 1.9791 - val_accuracy: 0.2000\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 48s 8s/step - loss: 1.9447 - accuracy: 0.1333 - val_loss: 1.9790 - val_accuracy: 0.2000\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 31s 5s/step - loss: 1.9374 - accuracy: 0.1444 - val_loss: 1.9739 - val_accuracy: 0.3000\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 29s 5s/step - loss: 1.8725 - accuracy: 0.2000 - val_loss: 1.8426 - val_accuracy: 0.3000\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 27s 4s/step - loss: 1.6852 - accuracy: 0.4000 - val_loss: 1.5349 - val_accuracy: 0.4000\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 26s 4s/step - loss: 1.4800 - accuracy: 0.6000 - val_loss: 1.2118 - val_accuracy: 0.6000\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 27s 4s/step - loss: 1.1023 - accuracy: 0.6778 - val_loss: 0.8806 - val_accuracy: 0.8000\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 26s 4s/step - loss: 0.8133 - accuracy: 0.8667 - val_loss: 0.6539 - val_accuracy: 0.8000\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 31s 5s/step - loss: 0.5269 - accuracy: 0.9222 - val_loss: 0.3934 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 27s 4s/step - loss: 0.3071 - accuracy: 0.9889 - val_loss: 0.2593 - val_accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "batch_size = 16\n",
    "epochs = 10 # Adjust according to your training needs\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_inputs,\n",
    "    train_labels,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.8365 - accuracy: 0.7600\n",
      "Test accuracy: 0.7599999904632568\n"
     ]
    }
   ],
   "source": [
    "#test model accuracy\n",
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_labels)\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to: ./saved_model\n"
     ]
    }
   ],
   "source": [
    "# Define directory to save the model\n",
    "output_dir = \"./saved_model\"\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Model saved successfully to:\", output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "# Load tokenizer and model\n",
    "def load_model(model_path):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    model = TFBertForSequenceClassification.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to classify a sentence\n",
    "def classify_sentence(tokenizer, model, sentence):\n",
    "    # Tokenize and encode the input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors='tf', padding=True, truncation=True)\n",
    "\n",
    "    # Make prediction\n",
    "    logits = model(inputs)[0]\n",
    "\n",
    "    # Get predicted class label\n",
    "    predicted_class = tf.argmax(logits, axis=1).numpy()[0]\n",
    "\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(sentence):\n",
    "    model_path = './saved_model'  # Path to the directory where you saved your trained model\n",
    "    tokenizer, model = load_model(model_path)\n",
    "\n",
    "    predicted_label = classify_sentence(tokenizer, model, sentence)\n",
    "    if predicted_label == 0:\n",
    "        print('you want to order a ticket')\n",
    "    elif predicted_label == 1:\n",
    "        print('you want to refund a ticket')\n",
    "    elif predicted_label == 2:\n",
    "        print('you want to check the status of your ticket')\n",
    "    elif predicted_label == 3:\n",
    "        print('you want to change the date of your ticket')\n",
    "    elif predicted_label == 4:\n",
    "        print('you want to change the destination of your ticket')\n",
    "    elif predicted_label == 5:\n",
    "        print('you want to know the weather of your destination')\n",
    "    else:\n",
    "        print('you want to know what is allowed in the flight')\n",
    "    print(\"Predicted Label:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./saved_model were not used when initializing TFBertForSequenceClassification: ['dropout_227']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ./saved_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you want to change the destination of your ticket\n",
      "Predicted Label: 4\n"
     ]
    }
   ],
   "source": [
    "main(\"אני רוצה לשנות את יעד הטיסה שלי לאיטליה\")\n",
    "# main(\"אני רוצה לדעת אם אני יכול לשנות את תאריך הטיסה שלי\")\n",
    "# main(\"אני רוצה לדעת אם אני יכול לבטל את הטיסה שלי\")\n",
    "# main(\"היי, הייתי רוצה להביא את החתול שלי לטיסה כי אני לא יכולה בלעדיו\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
