{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/classNERData.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an array of requests and entities\n",
    "requests = []\n",
    "entities = []\n",
    "for example in data:\n",
    "    requests.append(example[\"request\"])\n",
    "    entities.append(example['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for BIO format building\n",
    "def replace_words_with_indexes(sentence):\n",
    "    indexes = \"\"\n",
    "    counter=0\n",
    "    for word in sentence.split(\" \"):\n",
    "        indexes += str(counter) + \"-\" + str(counter + len(word)) + \" \"\n",
    "        counter += len(word) + 1\n",
    "    return indexes.rstrip()\n",
    "\n",
    "def create_BIO(data):\n",
    "    bio_labels = []\n",
    "    for item in data:\n",
    "        bio = [\"O\"] * len(item['request'].split())\n",
    "        indexes = replace_words_with_indexes(item['request'])\n",
    "        for entity in item['entities']:\n",
    "            start = entity['start']\n",
    "            end = entity['end']\n",
    "            label_type = entity['category']\n",
    "            for i, index in enumerate(indexes.split(\" \")):\n",
    "                if int(index.split(\"-\")[0]) >= start and int(index.split(\"-\")[1]) <= end:\n",
    "                    if int(index.split(\"-\")[0]) == start:\n",
    "                        bio[i] = 'B-' + label_type \n",
    "                    else:\n",
    "                        bio[i] = 'I-' + label_type\n",
    "        bio_labels.append(' '.join(bio))\n",
    "    return bio_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O O O O O O O', 'O O O B-DESTINATION', 'O O O O B-DESTINATION', 'O O O O O O O', 'O O O O B-ORIGIN I-ORIGIN B-DESTINATION I-DESTINATION O O', 'O O O O O O B-ORIGIN O B-DESTINATION B-DATE', 'O B-DATE I-DATE O O O B-DESTINATION', 'O O O O O O O O O', 'O O O O O O', 'O O O B-DESTINATION O B-DATE', 'O O O O O', 'O O O O O O', 'O O O O B-DESTINATION', 'O O O O O O O', 'O O O B-DESTINATION O O O', 'O O O O B-DESTINATION', 'O O O O O O', 'O O O B-DATE O', 'O O O O O', 'O O O O O O O']\n"
     ]
    }
   ],
   "source": [
    "#convert the entities to BIO format\n",
    "bio_labels = create_BIO(data)\n",
    "print(bio_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nizan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Tokenize the sentences\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"avichr/heBERT_NER\")\n",
    "# max_length = 64\n",
    "# tokenized_input = tokenizer(requests, truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert tokenized input and BIO labels to TensorFlow Dataset\n",
    "# def create_tf_dataset(tokenized_input, bio_labels):\n",
    "#     input_ids = tokenized_input['input_ids']\n",
    "#     attention_mask = tokenized_input['attention_mask']\n",
    "    \n",
    "#     dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "#         'input_ids': input_ids,\n",
    "#         'attention_mask': attention_mask\n",
    "#     }, {\n",
    "#         'bio_labels': bio_labels\n",
    "#     }))\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "# # Create TensorFlow Dataset\n",
    "# tf_dataset = create_tf_dataset(tokenized_input, bio_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_requests, val_requests, train_bio_labels, val_bio_labels = train_test_split(requests, bio_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "max_length = 64\n",
    "tokenizer = BertTokenizer.from_pretrained(\"avichr/heBERT_NER\")\n",
    "# Tokenize the training and validation sets\n",
    "train_tokenized_input = tokenizer(train_requests, truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
    "val_tokenized_input = tokenizer(val_requests, truncation=True, padding=True, max_length=max_length, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "4\n",
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(train_tokenized_input['input_ids']))\n",
    "print(len(val_tokenized_input['input_ids']))\n",
    "print(len(train_bio_labels))\n",
    "print(len(val_bio_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input ids shape: (16, 19)\n",
      "Train attention mask shape: (16, 19)\n",
      "Train labels shape: (16,)\n",
      "Validation input ids shape: (4, 9)\n",
      "Validation attention mask shape: (4, 9)\n",
      "Validation labels shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert tokenized input and BIO labels to NumPy arrays\n",
    "train_input_ids = np.array(train_tokenized_input['input_ids'])\n",
    "train_attention_mask = np.array(train_tokenized_input['attention_mask'])\n",
    "train_labels = np.array(train_bio_labels)\n",
    "\n",
    "val_input_ids = np.array(val_tokenized_input['input_ids'])\n",
    "val_attention_mask = np.array(val_tokenized_input['attention_mask'])\n",
    "val_labels = np.array(val_bio_labels)\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"Train input ids shape:\", train_input_ids.shape)\n",
    "print(\"Train attention mask shape:\", train_attention_mask.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "\n",
    "print(\"Validation input ids shape:\", val_input_ids.shape)\n",
    "print(\"Validation attention mask shape:\", val_attention_mask.shape)\n",
    "print(\"Validation labels shape:\", val_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\nizan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\nizan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\nizan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\nizan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\nizan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\nizan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 64), found shape=(None, 19)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 40\u001b[0m\n\u001b[0;32m     35\u001b[0m ner_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mner_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_attention_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_attention_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_labels\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nizan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file8dh0yhke.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\nizan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\nizan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\nizan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\nizan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\nizan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\nizan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 64), found shape=(None, 19)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "num_labels = 3\n",
    "num_epochs = 3\n",
    "\n",
    "# Define the model architecture\n",
    "def create_ner_model():\n",
    "    # Load pre-trained BERT model\n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    \n",
    "    # Freeze BERT layers\n",
    "    for layer in bert_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Input layer\n",
    "    input_ids = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    # BERT layer\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
    "    \n",
    "    # Classification layer\n",
    "    outputs = Dense(num_labels, activation='softmax')(bert_output)\n",
    "    \n",
    "    # Model\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the NER model\n",
    "ner_model = create_ner_model()\n",
    "\n",
    "# Compile the model\n",
    "ner_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = ner_model.fit(\n",
    "    [train_input_ids, train_attention_mask],\n",
    "    train_labels,\n",
    "    validation_data=(\n",
    "        [val_input_ids, val_attention_mask],\n",
    "        val_labels\n",
    "    ),\n",
    "    epochs=num_epochs\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
